{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "6. Weight Initialization of Layers.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "xPcixqKj5zOS"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import math as m\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model=4, num_heads=2, dropout=0.3):\n",
        "        super().__init__()\n",
        "\n",
        "        # d_q, d_k, d_v\n",
        "        self.d = d_model//num_heads\n",
        "\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        ##create a list of layers for K, and a list of layers for V\n",
        "        \n",
        "        self.linear_Qs = nn.ModuleList([nn.Linear(d_model, self.d)\n",
        "                                        for _ in range(num_heads)])\n",
        "        self.linear_Ks = nn.ModuleList([nn.Linear(d_model, self.d)\n",
        "                                        for _ in range(num_heads)])\n",
        "        self.linear_Vs = nn.ModuleList([nn.Linear(d_model, self.d)\n",
        "                                        for _ in range(num_heads)])\n",
        "\n",
        "        self.mha_linear = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
        "        # shape(Q) = [B x seq_len x D/num_heads]\n",
        "        # shape(K, V) = [B x seq_len x D/num_heads]\n",
        "\n",
        "        Q_K_matmul = torch.matmul(Q, K.permute(0, 2, 1))\n",
        "        scores = Q_K_matmul/m.sqrt(self.d)\n",
        "        # shape(scores) = [B x seq_len x seq_len]\n",
        "        \n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "        attention_weights = F.softmax(scores, dim=-1)\n",
        "        # shape(attention_weights) = [B x seq_len x seq_len]\n",
        "\n",
        "        output = torch.matmul(attention_weights, V)\n",
        "        # shape(output) = [B x seq_len x D/num_heads]\n",
        "\n",
        "        return output, attention_weights\n",
        "\n",
        "    def forward(self, pre_q, pre_k, pre_v, mask=None):\n",
        "        # shape(x) = [B x seq_len x D]\n",
        "\n",
        "        Q = [linear_Q(pre_q) for linear_Q in self.linear_Qs]\n",
        "        K = [linear_K(pre_k) for linear_K in self.linear_Ks]\n",
        "        V = [linear_V(pre_v) for linear_V in self.linear_Vs]\n",
        "        # shape(Q, K, V) = [B x seq_len x D/num_heads] * num_heads\n",
        "\n",
        "        \n",
        "\n",
        "        output_per_head = []\n",
        "        attn_weights_per_head = []\n",
        "        # shape(output_per_head) = [B x seq_len x D/num_heads] * num_heads\n",
        "        # shape(attn_weights_per_head) = [B x seq_len x seq_len] * num_heads\n",
        "        \n",
        "        for Q_, K_, V_ in zip(Q, K, V):\n",
        "            \n",
        "            ##run scaled_dot_product_attention\n",
        "            output, attn_weight = self.scaled_dot_product_attention(Q_, K_, V_, mask)\n",
        "            # shape(output) = [B x seq_len x D/num_heads]\n",
        "            # shape(attn_weights_per_head) = [B x seq_len x seq_len]\n",
        "            output_per_head.append(output)\n",
        "            attn_weights_per_head.append(attn_weight)\n",
        "\n",
        "        output = torch.cat(output_per_head, -1)\n",
        "        attn_weights = torch.stack(attn_weights_per_head).permute(1, 0, 2, 3)\n",
        "        # shape(output) = [B x seq_len x D]\n",
        "        # shape(attn_weights) = [B x num_heads x seq_len x seq_len]\n",
        "        \n",
        "        projection = self.dropout(self.mha_linear(output))\n",
        "\n",
        "        return projection, attn_weights\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class PWFFN(nn.Module):\n",
        "    def __init__(self, d_model, d_ff, dropout=0.3):\n",
        "        super().__init__()\n",
        "\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.Linear(d_model, d_ff),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(d_ff, d_model)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # shape(x) = [B x seq_len x D]\n",
        "\n",
        "        ff = self.ff(x)\n",
        "        # shape(ff) = [B x seq_len x D]\n",
        "\n",
        "        return ff"
      ],
      "metadata": {
        "id": "uKTrweeo627p"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class ResidualLayerNorm(nn.Module):\n",
        "    def __init__(self, d_model, dropout=0.3):\n",
        "        super().__init__()\n",
        "        self.layer_norm = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, residual):\n",
        "        ln = self.layer_norm(self.dropout(x) + residual)\n",
        "        return ln"
      ],
      "metadata": {
        "id": "43s2TjXm67Bo"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout=0.3, efficient_mha=False):\n",
        "        super().__init__()\n",
        "\n",
        "        # initalize these\n",
        "        self.norm_1 = ResidualLayerNorm(d_model, dropout)\n",
        "        self.norm_2 = ResidualLayerNorm(d_model, dropout)\n",
        "\n",
        "        self.mha = MultiHeadAttention(d_model, num_heads, dropout)\n",
        "\n",
        "        self.ff = PWFFN(d_model, d_ff, dropout)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        # shape(x) = [B x seq_len x D]\n",
        "\n",
        "        mha, encoder_attention_weights = self.mha(x, x, x, mask)\n",
        "        # shape(mha) = [B x seq_len x D]\n",
        "        # shape(encoder_attention_weights) = [B x num_heads x seq_len x seq_len]\n",
        "\n",
        "        norm1 = self.norm_1(mha, x)\n",
        "        # shape(norm1) = [B x seq_len x D]\n",
        "\n",
        "        ff = self.ff(norm1)\n",
        "        norm2 = self.norm_2(ff, norm1)\n",
        "        # shape(ff) = [B x seq_len x D]\n",
        "        # shape(norm2) = [B x seq_len x D]\n",
        "\n",
        "        return norm2, encoder_attention_weights"
      ],
      "metadata": {
        "id": "K22KsGjD7Acw"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "toy_encodings = torch.Tensor([[[0.0, 0.1, 0.2, 0.3], [1.0, 1.1, 1.2, 1.3], [2.0, 2.1, 2.2, 2.3]]]) \n",
        "# shape(toy_encodings) = [B, T, D] = (1, 3, 4)\n",
        "print(\"Toy Encodings:\\n\", toy_encodings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pd6hfEWH8qg4",
        "outputId": "84bf501d-4744-4ae9-93c6-a2e8f94fffa7"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Toy Encodings:\n",
            " tensor([[[0.0000, 0.1000, 0.2000, 0.3000],\n",
            "         [1.0000, 1.1000, 1.2000, 1.3000],\n",
            "         [2.0000, 2.1000, 2.2000, 2.3000]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "toy_encoder_layer = EncoderLayer(d_model=4,num_heads=2, d_ff=16)\n",
        "toy_encoder_layer_outputs, toy_encoder_layer_attn_outputs = toy_encoder_layer(toy_encodings,None)\n",
        "\n",
        "print(\"Encodings: \\n\", toy_encoder_layer_outputs)\n",
        "print(\"Encoder Layer Attn Weights: \\n\", toy_encoder_layer_attn_outputs)\n",
        "print(\"Encodings Shape: \\n\", toy_encoder_layer_outputs.shape)\n",
        "print(\"Encodings Attn Layer Weights Shape: \\n\", toy_encoder_layer_attn_outputs.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yua-8E438xX-",
        "outputId": "cef6b686-ff59-4529-a74f-2c7c2b91d99c"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encodings: \n",
            " tensor([[[ 0.8181, -1.6777,  0.1589,  0.7006],\n",
            "         [ 0.3733, -1.4753, -0.1829,  1.2848],\n",
            "         [ 0.9136, -1.5786, -0.1431,  0.8080]]],\n",
            "       grad_fn=<NativeLayerNormBackward0>)\n",
            "Encoder Layer Attn Weights: \n",
            " tensor([[[[0.2824, 0.3306, 0.3870],\n",
            "          [0.1588, 0.2946, 0.5466],\n",
            "          [0.0795, 0.2336, 0.6869]],\n",
            "\n",
            "         [[0.2697, 0.3290, 0.4013],\n",
            "          [0.1662, 0.2983, 0.5355],\n",
            "          [0.0941, 0.2487, 0.6572]]]], grad_fn=<PermuteBackward0>)\n",
            "Encodings Shape: \n",
            " torch.Size([1, 3, 4])\n",
            "Encodings Attn Layer Weights Shape: \n",
            " torch.Size([1, 2, 3, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Net = MultiHeadAttention()\n",
        "\n",
        "# Display all model layer weights\n",
        "for name, param in Net.named_parameters():\n",
        "    if param.requires_grad:\n",
        "        print(name, param.data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4USC-McHJorN",
        "outputId": "e0aacbf2-4542-48fe-897f-6c4a7ea23260"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "linear_Qs.0.weight tensor([[ 0.0060,  0.3131,  0.3927,  0.3131],\n",
            "        [ 0.3914, -0.3025,  0.4542, -0.1676]])\n",
            "linear_Qs.0.bias tensor([ 0.4214, -0.2699])\n",
            "linear_Qs.1.weight tensor([[ 0.4249, -0.2427, -0.0262, -0.1922],\n",
            "        [-0.4202, -0.4428,  0.1350,  0.0142]])\n",
            "linear_Qs.1.bias tensor([-0.2336, -0.4481])\n",
            "linear_Ks.0.weight tensor([[ 0.4329,  0.3307,  0.3568, -0.0264],\n",
            "        [ 0.3394, -0.4201, -0.0122,  0.1136]])\n",
            "linear_Ks.0.bias tensor([ 0.3852, -0.1858])\n",
            "linear_Ks.1.weight tensor([[-0.3380, -0.3888,  0.4956,  0.1191],\n",
            "        [-0.3206,  0.1198, -0.3628, -0.3521]])\n",
            "linear_Ks.1.bias tensor([0.0322, 0.2747])\n",
            "linear_Vs.0.weight tensor([[ 0.0266, -0.0121, -0.2924,  0.1973],\n",
            "        [ 0.2219,  0.1576,  0.1671, -0.0789]])\n",
            "linear_Vs.0.bias tensor([0.3774, 0.0881])\n",
            "linear_Vs.1.weight tensor([[-0.3803, -0.1506,  0.2279, -0.1779],\n",
            "        [-0.0215, -0.2624, -0.3260, -0.2322]])\n",
            "linear_Vs.1.bias tensor([-0.2906, -0.1739])\n",
            "mha_linear.weight tensor([[ 0.0193,  0.2813,  0.2510,  0.2645],\n",
            "        [ 0.1268, -0.1767,  0.3081,  0.2477],\n",
            "        [ 0.1797, -0.2884,  0.1672,  0.1771],\n",
            "        [-0.4785, -0.3191, -0.1360, -0.2157]])\n",
            "mha_linear.bias tensor([ 0.0266, -0.4775,  0.1243,  0.1812])\n"
          ]
        }
      ]
    }
  ]
}