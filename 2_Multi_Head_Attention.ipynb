{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2. Multi-Head Attention.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "b76aRq5nevv7"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math as m\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model=4, num_heads=2, dropout=0.3):\n",
        "        super().__init__()\n",
        "\n",
        "        # d_q, d_k, d_v\n",
        "        self.d = d_model//num_heads\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        ##create a list of layers for K, and a list of layers for V\n",
        "        \n",
        "        self.linear_Qs = nn.ModuleList([nn.Linear(d_model, self.d)\n",
        "                                        for _ in range(num_heads)])\n",
        "        self.linear_Ks = nn.ModuleList([nn.Linear(d_model, self.d)\n",
        "                                        for _ in range(num_heads)])\n",
        "        self.linear_Vs = nn.ModuleList([nn.Linear(d_model, self.d)\n",
        "                                        for _ in range(num_heads)])\n",
        "\n",
        "        self.mha_linear = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def scaled_dot_product_attention(self, Q, K, V):\n",
        "        # shape(Q) = [B x seq_len x D/num_heads] = [B x T x d_k]\n",
        "        # shape(K, V) = [B x T x d_k]\n",
        "\n",
        "        Q_K_matmul = torch.matmul(Q, K.permute(0, 2, 1))\n",
        "        scores = Q_K_matmul/m.sqrt(self.d)\n",
        "        # shape(scores) = [B x seq_len x seq_len]\n",
        "\n",
        "        attention_weights = F.softmax(scores, dim=-1)\n",
        "        # shape(attention_weights) = [B x seq_len x seq_len]\n",
        "\n",
        "        output = torch.matmul(attention_weights, V)\n",
        "        # shape(output) = [B x seq_len x D/num_heads]\n",
        "\n",
        "        return output, attention_weights\n",
        "\n",
        "    def forward(self, x):\n",
        "        # shape(x) = [B x seq_len x D]\n",
        "\n",
        "        Q = [linear_Q(x) for linear_Q in self.linear_Qs]\n",
        "        K = [linear_K(x) for linear_K in self.linear_Ks]\n",
        "        V = [linear_V(x) for linear_V in self.linear_Vs]\n",
        "        # shape(Q, K, V) = [B x seq_len x D/num_heads] * num_heads\n",
        "\n",
        "        output_per_head = []\n",
        "        attn_weights_per_head = []\n",
        "        # shape(output_per_head) = [B x seq_len x D/num_heads] * num_heads\n",
        "        # shape(attn_weights_per_head) = [B x seq_len x seq_len] * num_heads\n",
        "        for Q_, K_, V_ in zip(Q, K, V):\n",
        "            \n",
        "            ##run scaled_dot_product_attention\n",
        "            output, attn_weight = self.scaled_dot_product_attention(Q_, K_, V_)\n",
        "            # shape(output) = [B x seq_len x D/num_heads]\n",
        "            # shape(attn_weights_per_head) = [B x seq_len x seq_len]\n",
        "            output_per_head.append(output)\n",
        "            attn_weights_per_head.append(attn_weight)\n",
        "\n",
        "        output = torch.cat(output_per_head, -1)\n",
        "        attn_weights = torch.stack(attn_weights_per_head).permute(1, 0, 2, 3)\n",
        "        # shape(output) = [B x seq_len x D]\n",
        "        # shape(attn_weights) = [B x num_heads x seq_len x seq_len]\n",
        "        \n",
        "        projection = self.dropout(self.mha_linear(output))\n",
        "\n",
        "        return projection, attn_weights"
      ],
      "metadata": {
        "id": "wjXnG7h0e1yc"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "toy_encodings = torch.Tensor([[[0.0, 0.1, 0.2, 0.3], [1.0, 1.1, 1.2, 1.3], [2.0, 2.1, 2.2, 2.3]]]) \n",
        "# shape(toy_encodings) = [B, T, D] = (1, 3, 4)\n",
        "print(\"Toy Encodings:\\n\", toy_encodings)\n",
        "\n",
        "toy_MHA_layer = MultiHeadAttention(d_model=4, num_heads=2)\n",
        "toy_MHA, _ = toy_MHA_layer(toy_encodings)\n",
        "print(\"Toy MHA: \\n\", toy_MHA)\n",
        "print(\"Toy MHA Shape: \\n\", toy_MHA.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UuL2390Se6pE",
        "outputId": "27cbaafd-43c9-4af2-c421-b0c6297a539b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Toy Encodings:\n",
            " tensor([[[0.0000, 0.1000, 0.2000, 0.3000],\n",
            "         [1.0000, 1.1000, 1.2000, 1.3000],\n",
            "         [2.0000, 2.1000, 2.2000, 2.3000]]])\n",
            "Toy MHA: \n",
            " tensor([[[ 0.6675,  0.0000, -0.2364,  0.3230],\n",
            "         [ 0.0000,  0.0000, -0.2017,  0.3376],\n",
            "         [ 0.5644,  0.7681, -0.1686,  0.3516]]], grad_fn=<MulBackward0>)\n",
            "Toy MHA Shape: \n",
            " torch.Size([1, 3, 4])\n"
          ]
        }
      ]
    }
  ]
}