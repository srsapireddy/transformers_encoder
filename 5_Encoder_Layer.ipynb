{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "5. Encoder Layer.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "xPcixqKj5zOS"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import math as m\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model=4, num_heads=2, dropout=0.3):\n",
        "        super().__init__()\n",
        "\n",
        "        # d_q, d_k, d_v\n",
        "        self.d = d_model//num_heads\n",
        "\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        ##create a list of layers for K, and a list of layers for V\n",
        "        \n",
        "        self.linear_Qs = nn.ModuleList([nn.Linear(d_model, self.d)\n",
        "                                        for _ in range(num_heads)])\n",
        "        self.linear_Ks = nn.ModuleList([nn.Linear(d_model, self.d)\n",
        "                                        for _ in range(num_heads)])\n",
        "        self.linear_Vs = nn.ModuleList([nn.Linear(d_model, self.d)\n",
        "                                        for _ in range(num_heads)])\n",
        "\n",
        "        self.mha_linear = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
        "        # shape(Q) = [B x seq_len x D/num_heads]\n",
        "        # shape(K, V) = [B x seq_len x D/num_heads]\n",
        "\n",
        "        Q_K_matmul = torch.matmul(Q, K.permute(0, 2, 1))\n",
        "        scores = Q_K_matmul/m.sqrt(self.d)\n",
        "        # shape(scores) = [B x seq_len x seq_len]\n",
        "        \n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "        attention_weights = F.softmax(scores, dim=-1)\n",
        "        # shape(attention_weights) = [B x seq_len x seq_len]\n",
        "\n",
        "        output = torch.matmul(attention_weights, V)\n",
        "        # shape(output) = [B x seq_len x D/num_heads]\n",
        "\n",
        "        return output, attention_weights\n",
        "\n",
        "    def forward(self, pre_q, pre_k, pre_v, mask=None):\n",
        "        # shape(x) = [B x seq_len x D]\n",
        "\n",
        "        Q = [linear_Q(pre_q) for linear_Q in self.linear_Qs]\n",
        "        K = [linear_K(pre_k) for linear_K in self.linear_Ks]\n",
        "        V = [linear_V(pre_v) for linear_V in self.linear_Vs]\n",
        "        # shape(Q, K, V) = [B x seq_len x D/num_heads] * num_heads\n",
        "\n",
        "        output_per_head = []\n",
        "        attn_weights_per_head = []\n",
        "        # shape(output_per_head) = [B x seq_len x D/num_heads] * num_heads\n",
        "        # shape(attn_weights_per_head) = [B x seq_len x seq_len] * num_heads\n",
        "        \n",
        "        for Q_, K_, V_ in zip(Q, K, V):\n",
        "            \n",
        "            ##run scaled_dot_product_attention\n",
        "            output, attn_weight = self.scaled_dot_product_attention(Q_, K_, V_, mask)\n",
        "            # shape(output) = [B x seq_len x D/num_heads]\n",
        "            # shape(attn_weights_per_head) = [B x seq_len x seq_len]\n",
        "            output_per_head.append(output)\n",
        "            attn_weights_per_head.append(attn_weight)\n",
        "\n",
        "        output = torch.cat(output_per_head, -1)\n",
        "        attn_weights = torch.stack(attn_weights_per_head).permute(1, 0, 2, 3)\n",
        "        # shape(output) = [B x seq_len x D]\n",
        "        # shape(attn_weights) = [B x num_heads x seq_len x seq_len]\n",
        "        \n",
        "        projection = self.dropout(self.mha_linear(output))\n",
        "\n",
        "        return projection, attn_weights"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class PWFFN(nn.Module):\n",
        "    def __init__(self, d_model, d_ff, dropout=0.3):\n",
        "        super().__init__()\n",
        "\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.Linear(d_model, d_ff),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(d_ff, d_model)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # shape(x) = [B x seq_len x D]\n",
        "\n",
        "        ff = self.ff(x)\n",
        "        # shape(ff) = [B x seq_len x D]\n",
        "\n",
        "        return ff"
      ],
      "metadata": {
        "id": "uKTrweeo627p"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class ResidualLayerNorm(nn.Module):\n",
        "    def __init__(self, d_model, dropout=0.3):\n",
        "        super().__init__()\n",
        "        self.layer_norm = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, residual):\n",
        "        ln = self.layer_norm(self.dropout(x) + residual)\n",
        "        return ln"
      ],
      "metadata": {
        "id": "43s2TjXm67Bo"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout=0.3, efficient_mha=False):\n",
        "        super().__init__()\n",
        "\n",
        "        # initalize these\n",
        "        self.norm_1 = ResidualLayerNorm(d_model, dropout)\n",
        "        self.norm_2 = ResidualLayerNorm(d_model, dropout)\n",
        "\n",
        "        self.mha = MultiHeadAttention(d_model, num_heads, dropout)\n",
        "\n",
        "        self.ff = PWFFN(d_model, d_ff, dropout)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        # shape(x) = [B x seq_len x D]\n",
        "\n",
        "        mha, encoder_attention_weights = self.mha(x, x, x, mask)\n",
        "        # shape(mha) = [B x seq_len x D]\n",
        "        # shape(encoder_attention_weights) = [B x num_heads x seq_len x seq_len]\n",
        "\n",
        "        norm1 = self.norm_1(mha, x)\n",
        "        # shape(norm1) = [B x seq_len x D]\n",
        "\n",
        "        ff = self.ff(norm1)\n",
        "        norm2 = self.norm_2(ff, norm1)\n",
        "        # shape(ff) = [B x seq_len x D]\n",
        "        # shape(norm2) = [B x seq_len x D]\n",
        "\n",
        "        return norm2, encoder_attention_weights"
      ],
      "metadata": {
        "id": "K22KsGjD7Acw"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "toy_encodings = torch.Tensor([[[0.0, 0.1, 0.2, 0.3], [1.0, 1.1, 1.2, 1.3], [2.0, 2.1, 2.2, 2.3]]]) \n",
        "# shape(toy_encodings) = [B, T, D] = (1, 3, 4)\n",
        "print(\"Toy Encodings:\\n\", toy_encodings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pd6hfEWH8qg4",
        "outputId": "d43b5bbb-7914-4600-c5a0-ca43b1ebaf5a"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Toy Encodings:\n",
            " tensor([[[0.0000, 0.1000, 0.2000, 0.3000],\n",
            "         [1.0000, 1.1000, 1.2000, 1.3000],\n",
            "         [2.0000, 2.1000, 2.2000, 2.3000]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "toy_encoder_layer = EncoderLayer(d_model=4,num_heads=2, d_ff=16)\n",
        "toy_encoder_layer_outputs, toy_encoder_layer_attn_outputs = toy_encoder_layer(toy_encodings,None)\n",
        "\n",
        "print(\"Encodings: \\n\", toy_encoder_layer_outputs)\n",
        "print(\"Encoder Layer Attn Weights: \\n\", toy_encoder_layer_attn_outputs)\n",
        "print(\"Encodings Shape: \\n\", toy_encoder_layer_outputs.shape)\n",
        "print(\"Encodings Attn Layer Weights Shape: \\n\", toy_encoder_layer_attn_outputs.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yua-8E438xX-",
        "outputId": "96fb33f0-d9ef-49c3-9ca7-4275bfa433d1"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encodings: \n",
            " tensor([[[ 0.4009, -1.3086,  1.3798, -0.4722],\n",
            "         [ 0.3350, -0.1704,  1.3042, -1.4689],\n",
            "         [-1.1922,  1.4353, -0.6160,  0.3729]]],\n",
            "       grad_fn=<NativeLayerNormBackward0>)\n",
            "Encoder Layer Attn Weights: \n",
            " tensor([[[[0.3265, 0.3333, 0.3403],\n",
            "          [0.2811, 0.3304, 0.3885],\n",
            "          [0.2388, 0.3234, 0.4378]],\n",
            "\n",
            "         [[0.3952, 0.3297, 0.2750],\n",
            "          [0.4850, 0.3130, 0.2020],\n",
            "          [0.5719, 0.2856, 0.1426]]]], grad_fn=<PermuteBackward0>)\n",
            "Encodings Shape: \n",
            " torch.Size([1, 3, 4])\n",
            "Encodings Attn Layer Weights Shape: \n",
            " torch.Size([1, 2, 3, 3])\n"
          ]
        }
      ]
    }
  ]
}