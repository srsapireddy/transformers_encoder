{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1. Self-Attention.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "hPFAFGDPeQdX"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def scaled_dot_product_attention(Q, K, V, dk=4):\n",
        "    ##matmul Q and K\n",
        "    QK = torch.matmul(Q, K.T)\n",
        "    \n",
        "    ##scale QK by the sqrt of dk\n",
        "    matmul_scaled = QK / math.sqrt(dk)\n",
        "    \n",
        "    attention_weights = F.softmax(matmul_scaled, dim=-1)\n",
        "\n",
        "    ## matmul attention_weights by V\n",
        "    output = torch.matmul(attention_weights, V)\n",
        "\n",
        "    return output, attention_weights"
      ],
      "metadata": {
        "id": "S10uS_J5eYdF"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_attention(Q, K, V, n_digits = 3):\n",
        "    temp_out, temp_attn = scaled_dot_product_attention(Q, K, V)\n",
        "    temp_out, temp_attn = temp_out.numpy(), temp_attn.numpy()\n",
        "    print ('Attention weights are:')\n",
        "    print (np.round(temp_attn, n_digits))\n",
        "    print()\n",
        "    print ('Output is:')\n",
        "    print (np.around(temp_out, n_digits))\n",
        "\n",
        "# %%\n",
        "temp_k = torch.Tensor([[10,0,0],\n",
        "                      [0,10,0],\n",
        "                      [0,0,10],\n",
        "                      [0,0,10]])  # (4, 3)\n",
        "\n",
        "temp_v = torch.Tensor([[   1,0, 1],\n",
        "                      [  10,0, 2],\n",
        "                      [ 100,5, 0],\n",
        "                      [1000,6, 0]])  # (4, 3)"
      ],
      "metadata": {
        "id": "qwcTPzYIea0M"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This `query` aligns with the second `key`,\n",
        "# so the second `value` is returned.\n",
        "temp_q = torch.Tensor([[0, 10, 0]])  # (1, 3)\n",
        "print_attention(temp_q, temp_k, temp_v)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "khtZIXfVee_c",
        "outputId": "e5a307dc-18dc-4de9-bee1-bb23c83436bf"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention weights are:\n",
            "[[0. 1. 0. 0.]]\n",
            "\n",
            "Output is:\n",
            "[[10.  0.  2.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This query aligns with a repeated key (third and fourth), \n",
        "# so all associated values get averaged.\n",
        "temp_q = torch.Tensor([[0, 0, 10]])  # (1, 3)\n",
        "print_attention(temp_q, temp_k, temp_v)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s93nBV5veg2k",
        "outputId": "d130e6b7-fcb1-44ba-9acf-6a08569f57b8"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention weights are:\n",
            "[[0.  0.  0.5 0.5]]\n",
            "\n",
            "Output is:\n",
            "[[550.    5.5   0. ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This query aligns equally with the first and second key, \n",
        "# so their values get averaged.\n",
        "temp_q = torch.Tensor([[10, 10, 0]])  # (1, 3)\n",
        "print_attention(temp_q, temp_k, temp_v)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IkzCDnQuehX1",
        "outputId": "5a02a2e0-6296-4ca5-8ac8-a80f1abefaa0"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention weights are:\n",
            "[[0.5 0.5 0.  0. ]]\n",
            "\n",
            "Output is:\n",
            "[[5.5 0.  1.5]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "temp_q = torch.Tensor([[0, 10, 0], [0, 0, 10], [10, 10, 0]])  # (3, 3)\n",
        "print_attention(temp_q, temp_k, temp_v)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SW-u_Hrbejtl",
        "outputId": "a8386534-6e9d-4a11-b85c-1ddaeb9b15f0"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention weights are:\n",
            "[[0.  1.  0.  0. ]\n",
            " [0.  0.  0.5 0.5]\n",
            " [0.5 0.5 0.  0. ]]\n",
            "\n",
            "Output is:\n",
            "[[ 10.    0.    2. ]\n",
            " [550.    5.5   0. ]\n",
            " [  5.5   0.    1.5]]\n"
          ]
        }
      ]
    }
  ]
}