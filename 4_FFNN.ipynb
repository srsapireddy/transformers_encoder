{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "4. FFNN.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "AKJfVwx2DxOc"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class PWFFN(nn.Module):\n",
        "    def __init__(self, d_model, d_ff, dropout=0.3):\n",
        "        super().__init__()\n",
        "\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.Linear(d_model, d_ff),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(d_ff, d_model)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # shape(x) = [B x seq_len x D]\n",
        "\n",
        "        ff = self.ff(x)\n",
        "        # shape(ff) = [B x seq_len x D]\n",
        "\n",
        "        return ff"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "\n",
        "\n",
        "class ResidualLayerNorm(nn.Module):\n",
        "    def __init__(self, d_model, dropout=0.3):\n",
        "        super().__init__()\n",
        "        self.layer_norm = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, residual):\n",
        "        ln = self.layer_norm(self.dropout(x) + residual)\n",
        "        return ln"
      ],
      "metadata": {
        "id": "yY35wfKbEXgY"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encodings = torch.Tensor([[[0.0, 0.1, 0.2, 0.3], [1.0, 1.1, 1.2, 1.3], [2.0, 2.1, 2.2, 2.3]]]) \n",
        "prev_x = torch.randn(1, 3, 4)\n",
        "norm_layer = ResidualLayerNorm(d_model=4)\n",
        "norm = norm_layer(encodings, prev_x)\n",
        "print(\"Norm: \\n\", norm)\n",
        "print(\"Norm shape: \\n\", norm.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yv21_1Y_EacZ",
        "outputId": "02dc7cda-0248-4397-8b54-873673c2460b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Norm: \n",
            " tensor([[[-1.5965,  0.5186,  1.0873, -0.0094],\n",
            "         [ 0.4455, -1.5694, -0.0326,  1.1565],\n",
            "         [-0.9016,  1.1520, -1.0814,  0.8310]]],\n",
            "       grad_fn=<NativeLayerNormBackward0>)\n",
            "Norm shape: \n",
            " torch.Size([1, 3, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PWFFN_layer = PWFFN(d_model=4, d_ff=16)\n",
        "PWFFN = PWFFN_layer(norm)\n",
        "print(\"PWFFN: \\n\", PWFFN)\n",
        "print(\"PWFFN Shape: \\n\", PWFFN.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "erV2HDbyECVl",
        "outputId": "710be84c-1a0b-46c0-870a-c315170960ee"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PWFFN: \n",
            " tensor([[[-0.0035, -0.2865, -0.0963, -0.2456],\n",
            "         [-0.0009, -0.3834,  0.0740, -0.0520],\n",
            "         [ 0.2627, -0.1414, -0.1472, -0.1193]]], grad_fn=<AddBackward0>)\n",
            "PWFFN Shape: \n",
            " torch.Size([1, 3, 4])\n"
          ]
        }
      ]
    }
  ]
}